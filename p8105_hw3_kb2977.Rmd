---
title: "p8105_hw3_kb2977"
author: "Francois Ban"
date: "10/7/2018"
output: github_document
---

##Problem 1

###Part 1: Loading brfss_data and cleaning

1. Format the data to use appropriate variable names
2. Focus on the “Overall Health” topic
3. Include only responses from “Excellent” to “Poor” (i.e. no pre-collapsed categories)
4. Organize responses as a factor taking levels from “Excellent” to “Poor”
5. Getting rid of unnecesary variables

```{r brfss_data}
library(tidyverse)

devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
data(brfss_smart2010)

brfss_data = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health" & response == "Excellent" | response == "Poor" | response == "Very good" | response == "Good" | response == "Fair") %>%
  mutate(response = as.factor(response)) %>% 
  select(-class, -topic, -question, -sample_size, -confidence_limit_low:-geo_location)
```

###Part 2: Answering following questions

Using the dataset above:

* In 2002, which states were observed at 7 locations?

```{r brfss_data_2002}
brfss_data_2002 = 
  brfss_data %>%
  spread(key = response, value = data_value) %>%
  filter(year == "2002") %>%
  count(locationabbr) %>% 
  filter(n == 7)
brfss_data_2002
```

* Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

```{r state_plot}
state_plot = brfss_data %>% 
  group_by(locationabbr, year) %>% 
  summarize(n_obs = n()) %>%
  ggplot(aes(x = year, y = n_obs, color = locationabbr)) +
    geom_line()
state_plot


knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "100%"
)
```

* Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r table_data}
table_data = 
  brfss_data %>%
  filter(year == "2002" | year == "2006" | year == "2010") %>% 
  filter(response == "Excellent") %>%
  filter(locationabbr == "NY") %>%
  group_by(year) %>%
  summarize(mean_ex = mean(data_value),
            sd_ex = sd(data_value)) %>%
  knitr::kable(digits = 1)
```

* For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r plot}
devtools::install_github("thomasp85/patchwork")
library(patchwork)
library(ggridges)

av_prop_plot =
  brfss_data %>% 
  spread(key = response, value = data_value) %>% 
  group_by(year, locationabbr) %>% 
  summarize(av_prop_excellent = mean(Excellent),
            av_prop_verygood = mean(`Very good`),
            av_prop_good = mean(Good),
            av_prop_fair = mean(Fair),
            av_prop_poor = mean(Poor)) %>% 
  gather(key = response, value = mean, av_prop_excellent:av_prop_poor) %>% 
  separate(response, into = c("remove_1", "remove_2", "response"), sep = "_") %>% 
  select(-remove_1,-remove_2) %>% 
  ggplot(aes(x = year, y = mean, color = locationabbr)) +
  geom_point(alpha = .5) +
  facet_grid(~response) +
  labs(
    title = "Distribution of State-Level Responses",
    x = "Time in years",
    y = "Average proportion of responses",
    caption = "Data from the rnoaa package"
  ) +
  scale_color_hue(name = "States")
av_prop_plot
```

##Problem 2

```{r instacart_data}
data(instacart)
```

###Part 1: Exploration of instacart dataset

Directions: Write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illustrative examples of observations. 

* There were orders placed.
* The average number of items ordered in this dataset is
* The user_id with the most orders was
* The item that was the most ordered was 
* The item that was the most reordered was
* Customers tend to order on ___ day at around ___ time.

###Part 2: Answering questions using instacart_data

1. How many aisles are there, and which aisles are the most items ordered from? There were a total of `r count(distinct(instacart, aisle_id))` aisles. 

2. The following aisles had the most items ordered:
`r instacart %>% count(aisle) %>% top_n(5) %>% knitr::kable(digits = 1)`

3. Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r}
items_ordered = 
  instacart %>% 
  count(aisle_id) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(x = aisle_id, y = n)) +
  geom_point(alpha = 0.5, size = .8) +
  labs(
    title = "Plot of Items Ordered in Each Aisle",
    x = "Aisle ID",
    y = "Number of items",
    caption = "Data from the instacart package"
  )
items_ordered
```


4. Make a table showing the most popular item in the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r pop_item}
pop_item = instacart

```


5. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

##Problem 3

```{r ny_noaa}
data(ny_noaa)
```

###Part 1: Exploration of ny_noaa dataset

Directions: Write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. 

*

###Part 2: Answering questions using ny_noaa datset

1. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.

```{r}
ny_noaa_data = 
  ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = as.numeric(prcp)/10)
```

For snowfall, the most commonly observed value was `r names(which.max(table(ny_noaa_data$snow)))`. This is because most days of the year, it does not snow.

2. Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

3. Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
